---
title: "Classifiation with Slice Sampling"
author: "Andreas Hofheinz"
date: "27 11 2020"
output: pdf_document
---

```{r, warning=FALSE, message=FALSE}
library(bisemSliceSampler)
library(latex2exp)
library(patchwork)
library(ggplot2)
```

```{r, echo=FALSE}
# from https://stackoverflow.com/questions/23891140/r-how-to-visualize-confusion-matrix-using-the-caret-package/42940553
draw_confusion_matrix <- function(cm) {

  total <- sum(cm$table)
  res <- as.numeric(cm$table)

  # Generate color gradients. Palettes come from RColorBrewer.
  greenPalette <- c(
    "#F7FCF5", "#E5F5E0", "#C7E9C0", "#A1D99B", "#74C476",
    "#41AB5D", "#238B45", "#006D2C", "#00441B"
  )
  redPalette <- c(
    "#FFF5F0", "#FEE0D2", "#FCBBA1", "#FC9272", "#FB6A4A", 
    "#EF3B2C", "#CB181D", "#A50F15", "#67000D"
  )
  getColor <- function(greenOrRed = "green", amount = 0) {
    if (amount == 0) {
      return("#FFFFFF")
    }
    palette <- greenPalette
    if (greenOrRed == "red") {
      palette <- redPalette
    }
    colorRampPalette(palette)(100)[10 + ceiling(90 * amount / total)]
  }

  # set the basic layout
  layout(matrix(c(1, 1, 2)))
  par(mar = c(2, 2, 2, 2))
  plot(c(100, 345), c(300, 450), type = "n", xlab = "", ylab = "", xaxt = "n", yaxt = "n")
  title("CONFUSION MATRIX", cex.main = 2)

  # create the matrix
  classes <- colnames(cm$table)
  rect(150, 430, 240, 370, col = getColor("green", res[1]))
  text(195, 435, classes[1], cex = 1.2)
  rect(250, 430, 340, 370, col = getColor("red", res[3]))
  text(295, 435, classes[2], cex = 1.2)
  text(125, 370, "Predicted", cex = 1.3, srt = 90, font = 2)
  text(245, 450, "Actual", cex = 1.3, font = 2)
  rect(150, 305, 240, 365, col = getColor("red", res[2]))
  rect(250, 305, 340, 365, col = getColor("green", res[4]))
  text(140, 400, classes[1], cex = 1.2, srt = 90)
  text(140, 335, classes[2], cex = 1.2, srt = 90)

  # add in the cm results
  text(195, 400, res[1], cex = 1.6, font = 2, col = "white")
  text(195, 335, res[2], cex = 1.6, font = 2, col = "white")
  text(295, 400, res[3], cex = 1.6, font = 2, col = "white")
  text(295, 335, res[4], cex = 1.6, font = 2, col = "white")

  # add in the specifics
  plot(c(100, 0), c(100, 0), type = "n", xlab = "", ylab = "", main = "DETAILS", xaxt = "n", yaxt = "n")
  text(10, 85, names(cm$byClass[1]), cex = 1.2, font = 2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex = 1.2)
  text(30, 85, names(cm$byClass[2]), cex = 1.2, font = 2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex = 1.2)
  text(50, 85, names(cm$byClass[5]), cex = 1.2, font = 2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex = 1.2)
  text(70, 85, names(cm$byClass[6]), cex = 1.2, font = 2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex = 1.2)
  text(90, 85, names(cm$byClass[7]), cex = 1.2, font = 2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex = 1.2)

  # add in the accuracy information
  text(30, 35, names(cm$overall[1]), cex = 1.5, font = 2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex = 1.4)
  text(70, 35, names(cm$overall[2]), cex = 1.5, font = 2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex = 1.4)
}
```



```{r, warning=FALSE, message=FALSE}
data(PimaIndiansDiabetes , package  = "mlbench")
diabetis_sub <- subset(PimaIndiansDiabetes, select = -c(triceps, insulin))
diabetis_sub$diabetes <- as.numeric(diabetis_sub$diabetes) - 1
```

```{r}
n = nrow(diabetis_sub)
trainIndex = sample(1:n, size = round(0.7*n), replace = FALSE)
train = diabetis_sub[trainIndex, ]
test = diabetis_sub[-trainIndex, ]
```


```{r}
# Scaling speeds up convergence
X <- scale(as.matrix(subset(train, select = -diabetes)))
y <- train$diabetes
```

**Model:** 

- Verteilungsannahme: $y_{i} \mid \boldsymbol{x_{i}} \stackrel{\text {unabh.}}{\sim} \operatorname{Bin}\left(1, \pi_{i}\right)$

- Linearer PrÃ¤diktor: $\quad \eta_{i}=\boldsymbol{x}_{i}^{\top} \boldsymbol{\beta}$

- Responsefunktion: $\quad \mathbb{E}\left(y_{i} \mid x_{i}\right)=\pi_{i}=h\left(\eta_{i}\right)=\frac{\exp \left(\eta_{i}\right)}{1+\exp \left(\eta_{i}\right)}$

- Features: $\boldsymbol{x_{i}} = (x_{\mathrm{pregnant}, i}, \; x_{\mathrm{glucose}, i}, \; x_{\mathrm{pressure}, i}, x_{\mathrm{mass}, i}, \; x_{\mathrm{pedigree}, i}, \; x_{\mathrm{age}, i})$

\bigskip 


**Likelihood:**

$L\left(\boldsymbol{\beta}\right)= \prod_{i=1}^{n} (h({\boldsymbol{x}_i}^T\boldsymbol{\beta})) ^{yi}(1-h({\boldsymbol{x}_i}^T\boldsymbol{\beta}))^{1-y_{i}}$

\bigskip 

**Prior**

In this example we specify a noninformative prior: \ $\boldsymbol{\beta} \sim N(\boldsymbol{0}, 1000 * I)$
such that $p\left(\boldsymbol{\beta}\right) \propto  \exp \left(-\frac{1}{2*1000} \boldsymbol{\beta}^{\top} \boldsymbol{\beta}\right)$

\bigskip 


**Posterior**

$$
p\left(\boldsymbol{\beta} \mid \boldsymbol{y} \right)
\propto \left[\prod_{i=1}^{n} (h({\boldsymbol{x}_i}^T\boldsymbol{\beta})) ^{yi}(1-h({\boldsymbol{x}_i}^T\boldsymbol{\beta}))^{1-y_{i}}\right] * \exp \left(-\frac{1}{2000} \boldsymbol{\beta}^{\top} \boldsymbol{\beta}\right)
$$

**Log-Posterior**

$$
log(p\left(\boldsymbol{\beta} \mid \boldsymbol{y} \right)) 
\propto \left(\sum_{i=1}^{n} y_{i}\right) \log h({\boldsymbol{x}_i}^T\boldsymbol{\beta})+\left(n-\sum_{i=1}^{n} y_{i}\right) \log (1-h({\boldsymbol{x}_i}^T\boldsymbol{\beta})) -\frac{\boldsymbol{\beta}^{\top} \boldsymbol{\beta}}{2000} 
$$

```{r}
h <- function(x) exp(x) / (1 + exp(x))

logpost <- function(beta) {
  pi  <- h(X %*% beta)
  y %*% log(pi) + (1 - y) %*% log(1 - pi) - (1 / 2000) * crossprod(beta)
}
```


\bigskip 


**Sampling from Log-Posterior**

```{r}
w <- rep(1, times = ncol(X))
n_samples <- 2000
beta_init <- rep(0, ncol(X))
```


```{r, message=FALSE}
beta_samples  <- slice_sampler(
  logpost , x_init = beta_init, w = w, n_samples = n_samples
)
```

\newpage

**Convergence Diagnostic**

```{r, fig.height=8, echo=FALSE}
burn_in <- 300

# Graph Sample Number vs Value
df_beta_samples <- as.data.frame(beta_samples)
names(df_beta_samples) <- paste0("beta", 1:6)
df_beta_samples$samples <- 1:nrow(beta_samples)
df_tidy <- df_beta_samples %>% 
  tidyr::pivot_longer(beta1:beta6, names_to = "coef") %>% 
  dplyr::mutate(coef = as.factor(coef))

levels(df_tidy$coef) <- c(
  beta1  = TeX("$\\beta_1$"),
  beta2  = TeX("$\\beta_2$"),
  beta3  = TeX("$\\beta_3$"),
  beta4  = TeX("$\\beta_4$"), 
  beta5  = TeX("$\\beta_5$"),
  beta6  = TeX("$\\beta_6$")
)

ggplot(df_tidy, aes(samples , value)) + 
  geom_line() + 
  facet_wrap(~coef, ncol = 2, scales = "free_y", labeller = label_parsed) +
  geom_vline(xintercept = burn_in, color = "red", linetype = "dashed") +
  theme_bw()
```


\newpage

**Convergence Diagnostic - 300 burn-in samples removed**

```{r, fig.height=8, echo=FALSE}
# Remove samples for sigma_sq and 300 burn-in
beta_samples_no_burnin <- beta_samples[(burn_in + 1):nrow(beta_samples), ]

# Graph Sample Number vs Value
df <- as.data.frame(beta_samples_no_burnin)
names(df) <- paste0("beta", 1:6)
df$samples <- 1:nrow(beta_samples_no_burnin)
df_tidy <- df %>% 
  tidyr::pivot_longer(beta1:beta6, names_to = "coef") %>% 
  dplyr::mutate(coef = as.factor(coef))

levels(df_tidy$coef) <- c(
  beta1  = TeX("$\\beta_1$"),
  beta2  = TeX("$\\beta_2$"),
  beta3  = TeX("$\\beta_3$"),
  beta4  = TeX("$\\beta_4$"), 
  beta5  = TeX("$\\beta_5$"),
  beta6  = TeX("$\\beta_6$")
)

ggplot(df_tidy, aes(samples , value)) +
  geom_line() +
  facet_wrap(~coef, ncol = 2, scales = "free_y", labeller = label_parsed) +
  theme_bw()
```


\newpage

**Coefficients and Credibility Intervals**

```{r, echo=FALSE, comment=""}
summary_raw <- t(apply(beta_samples_no_burnin, 2, function(x) {
  c(
    round(mean(x), 3),
    round(quantile(x, prob = 0.025), 3),
    round(quantile(x, prob = 0.975), 3)
  )
}))
summary_df <-data.frame(summary_raw)
rownames(summary_df) <- colnames(X)
colnames(summary_df) <- c("Mean", "2.5%", "97.5%")
summary_df
```

**Comparison with frequentist calculation**

Expected to be similar since non-informative prior was chosen.

```{r, echo=FALSE, comment=""}
myglm <- glm(formula = y ~ X -1, family = binomial)
summary_myglm <- as.data.frame(cbind(myglm$coefficients, confint(myglm)))
summary_myglm <- round(summary_myglm, 3)
rownames(summary_myglm) <- substring(rownames(summary_myglm), 2)
colnames(summary_myglm) <- c("Mean", "2.5%", "97.5%")
summary_myglm
```

**Posterior predictive**

Idea for presentation: 
  - Maybe explain for one sample first
  - Add a bit more formulas


$p(\tilde{y} \mid y) =\int p(\tilde{y} \mid \boldsymbol{\beta}) p(\boldsymbol{\beta} \mid y) d \boldsymbol{\beta}$

```{r}
X_test <- scale(as.matrix(subset(test, select = -diabetes)))
```

```{r}
# For each new obs we have 1700 eta (2000 - burn_in)
eta <- X_test %*% t(beta_samples_no_burnin)
pi <- h(eta)
```

```{r}
# For each new obs i: for all (1700) pi_i : draw y ~ Bin(1, pi_i)
post_pred_y <- matrix(NA, nrow = nrow(X_test), ncol = ncol(pi))
for (i in 1:nrow(X_test)) {
  post_pred_y[i, ] <- rbinom(ncol(pi), 1, prob = pi[i, ])
}
```

```{r}
dim(post_pred_y)
```

Point estimator for $\tilde{y_i}$: 

$\hat{\tilde{y_i}} = \frac{1}{1700} * \sum_{k=1}^{1700} \tilde{y}_{i,k}$

```{r, echo=FALSE}
y_hat <- rowMeans(post_pred_y)
```

```{r, echo=FALSE}
y_true <- test$diabetes
cm1 <- caret::confusionMatrix(
  data = as.factor(as.numeric(y_hat > 0.5)), 
  reference = as.factor(y_true)
)
draw_confusion_matrix(cm1)
```


```{r, echo=FALSE, message=FALSE, fig.height = 2}
res_roc <- pROC::roc(y_true, y_hat)
roc_data <- data.frame(
  thresholds = res_roc$thresholds,
  sensitivity = res_roc$sensitivities,
  specificity = res_roc$specificities
)

ggplot(roc_data, aes(specificity, sensitivity)) + 
  geom_path(color = "blue")+
  scale_x_reverse(expand = c(0,0.01)) +
  scale_y_continuous(expand = c(0,0)) +
  geom_abline(intercept = 1, slope = 1, linetype = "dashed") +
  ggtitle("ROC curve") + 
  theme_bw()
```

\newpage

**Comparison with GLM**

```{r, echo=FALSE}
glm_data <- data.frame(y = y, train)
glm_data$diabetes <- NULL 
myglm <- glm(formula = y ~ . -1, family = binomial, data = glm_data)
glm_test <- test
glm_test$diabetes <- NULL
glm_test$intercept <- 1
glm_preds <- predict(myglm, newdata = glm_test, type = "response")

cm2 <- caret::confusionMatrix(data = as.factor(as.numeric(glm_preds > 0.5)), reference = as.factor(y_true))
draw_confusion_matrix(cm2)
```

```{r, echo=FALSE, message=FALSE, fig.height = 3}
res_roc_glm <- pROC::roc(y_true, glm_preds)
roc_data_glm <- data.frame(
  thresholds = res_roc_glm$thresholds,
  sensitivity = res_roc_glm$sensitivities,
  specificity = res_roc_glm$specificities
)

ggplot(roc_data_glm, aes(specificity, sensitivity)) + 
  geom_path(color = "blue")+
  scale_x_reverse(expand = c(0,0.01)) +
  scale_y_continuous(expand = c(0,0)) +
  geom_abline(intercept = 1, slope = 1, linetype = "dashed") +
  ggtitle("ROC curve") + 
  theme_bw()
```
